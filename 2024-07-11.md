## [[NEM]]
GOALS: 
- Understand whether better initialization might aid in the results.  I would assume that a base initialization around 0 masking give the model more time to derive better results
- How to control for distance across different model latent spaces. 
- Is cosine distance actually a good measurement method.

I want to understand why 
If we only use the base loss function, we get very unranked (dense) and small explanations.
base loss function is defined as L = MR + Pdist.
It is kinda weird that background is also not discarded. 
if we add the kulback leibler divergence, it means that we penalize all probs, which are far from the center prob.... So would that even make sense. 
**!** Turns out I had activated stochastic neighbourhood averaging. So it make sense that the observations become quite dense. I will rerun the experiments.  
It seems to have improved results by a fair margin. I am still annoyed by the collapse. 
I have to find a way to rank the features, which have not been discarded.
I guess the clever thing about the KL divergence is that it penalizes the value of individual features and not the overall level of penalization.  I wonder if that is the key to more balanced results. Using a scaled variant of the KL divergence instead of the masking penalty might actually be more elegant. Note that the max KL divergence between two Bernoulli random variables are 8. So if I set bernoulli to 0.0001
Swav Still create very small explanations with the normal loss, but they are not necessarily bad.
*Experiment: use KL instead of MR*


