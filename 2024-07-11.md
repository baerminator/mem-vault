## [[NEM]]
GOALS: 
- Understand whether better initialization might aid in the results.  I would assume that a base initialization around 0 masking give the model more time to derive better results
- How to control for distance across different model latent spaces. 
- Is cosine distance actually a good measurement method.

I want to understand why 
If we only use the base loss function, we get very unranked (dense) and small explanations.
base loss function is defined as L = MR + Pdist.
It is kinda weird that background is also not discarded. 
if we add the kulback leibler divergence, it means that we penalize all probs, which are far from the center prob.... So would that even make sense. 
**!** Turns out I had activated stochastic neighbourhood averaging. So it make sense that the observations become quite dense. I will rerun the experiments.  
It seems to have improved results by a fair margin. I am still annoyed by the collapse. 
I have to find a way to rank the features, which have not been discarded.
I guess the clever thing about the KL divergence is that it penalizes the value of individual features and not the overall level of penalization.  I wonder if that is the key to more balanced results. Using a scaled variant of the KL divergence instead of the masking penalty might actually be more elegant. Note that the max KL divergence between two Bernoulli random variables are 8. So if I set bernoulli to 0.0001
Swav Still create very small explanations with the normal loss, but they are not necessarily bad.
*Experiment: use KL(p = 0.0001) instead of MR*
Using the method on SWAV and scaling the KL with 1/8 creates almost no masking . hmm
**!** I am an idiot... forgot to turn the KL on.... rerunning experiment.
*Experiment: use KL(p = 0.0001) instead of MR*, SWAV.
One interesting not is withouth KL it still masked something out. indicating that initialization needs to be updated to full masking.
Thinking  about whether we should update the distance measure, since there is probably some noise in it from all the random shit going on with the final trained model. 
... Actually initializing from completely masked might be an interesting idea... All is removed and we only find the important things. Would that even work ? 
*results* 
Actually not bad. Distance is 0.58 at MR 0.065. will try to run with scaling 1/10.
Only real issue is that it finds weird artifacts in the top ( there is a line)
*Experiment: use KL(p = 0.0001) instead of MR*, scale 1/10, SWAV.
*results* 
Like the results before a bit more. But the 1/8 seems more natural honestly. The general metrics are also better... I try to rerun the experiments. I would like it to be 1/10 since it seems more clean XD 
*Experiment: use KL(p = 0.0001) instead of MR*, scale 1/10, SWAV.
*results*
ended up bad because of a line artifact in a corner....  I will try to instantiate the output mask to 1. 
*Experiment: use KL(p = 0.0001) instead of MR*, scale 1/10, SWAV last output layer bias = 5
*results* 
better. rerunning to check non deterministic effects. 
*Experiment: use KL(p = 0.0001) instead of MR*, scale 1/10, SWAV last output layer bias = 5
*results* 
yep. seems to have solved artifacts. Time to try some other methods huh. SWAV does not like the cats ... 
*Experiment: use KL(p = 0.0001) instead of MR*, scale 1/10, SIMCLR last output layer bias = 5
*results* 
Seems okay, but the explanations could be a bit smaller. trying the 1/8.
*Experiment: use KL(p = 0.0001) instead of MR*, scale 1/8, SIMCLR last output layer bias = 5
*results* 
Seems awfully a lot like the ones from the normal masking. dang. trying with normal masking 
MR, SIMCLR last output layer bias = 5
*results* 
also seems okay. a small decrease, but this could probably be tuned away.  But actually the varians with KL seems a bit better. 
will try to set output layer bias  = -5 
MR, SIMCLR last output layer bias = -5
*results* 